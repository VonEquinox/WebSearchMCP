{
  "success": true,
  "url": "https://trafilatura.readthedocs.io/en/latest/usage-python.html",
  "via_worker": false,
  "via_playwright": false,
  "status_code": 200,
  "text": "With Python#\nThe Python programming language#\nStep-by-step#\nQuickstart#\nFor the basics see quickstart documentation page.\nExtraction functions#\nfrom trafilatura import ...\nand used on raw documents (strings) or parsed HTML (LXML elements).\nMain text extraction, good balance between precision and recall:\nextract\n: Wrapper function, easiest way to perform text extraction and conversionbare_extraction\n: Internal function returning bare Python variables\nAdditional fallback functions:\nbaseline\n: Faster extraction function targeting text paragraphs and/or JSON metadatahtml2txt\n: Extract all text in a document, maximizing recall\nOutput#\nCSV\nHTML (from version 1.11 onwards)\nJSON\nMarkdown (from version 1.9 onwards)\nXML and XML-TEI (following the guidelines of the Text Encoding Initiative)\n\"csv\", \"json\", \"html\", \"markdown\", \"txt\", \"xml\", \"xmltei\"\n.\nbare_extraction\nfunction also accepts an additional python\nformat to work with Python on the output.\nTo extract and include metadata in the output, use the with_metadata=True\nargument.\nExamples#\n# some formatting preserved in basic XML structure\n>>> extract(downloaded, output_format=\"xml\")\n# output in JSON format with metadata extracted\n>>> extract(downloaded, output_format=\"json\", with_metadata=True)\nChoice of HTML elements#\nCustomize the extraction process by including or excluding specific HTML elements:\n- Text elements:\ninclude_comments=True\nInclude comment sections at the bottom of articles.\ninclude_tables=True\nExtract text from HTML\n<table>\nelements.\n- Structural elements:\ninclude_formatting=True\nKeep structural elements related to formatting (\n<b>\n/<strong>\n,<i>\n/<emph>\netc.)include_links=True\nKeep link targets (in\nhref=\"...\"\n)include_images=True\nKeep track of images along with their targets (\n<img>\nattributes: alt, src, title)\nTo operate on these elements, pass the corresponding parameters to the extract()\nfunction:\n# exclude comments from the output\n>>> result = extract(downloaded, include_comments=False)\n# skip tables and include links in the output\n>>> result = extract(downloaded, include_tables=False, include_links=True)\n# convert relative links to absolute links where possible\n>>> extract(downloaded, output_format='xml', include_links=True, url=url)\nImportant notes#\ninclude_comments\nandinclude_tables\nare activated by default.bare_extraction()\n. This allows for direct display and manipulation of the elements.Certain elements may not be visible in the output if the chosen format does not allow it.\nSelecting Markdown automatically includes text formatting.\nHint\nThe precision and recall presets#\nfavor_precision\nand favor_recall\n.\n>>> result = extract(downloaded, url, favor_precision=True)\nPrecision#\nprune_xpath\nparameter to target specific HTML elements using a list of XPath expressions.\nRecall#\nIf parts of your documents are missing, try this preset to take more elements into account.\nIf content is still missing, refer to the troubleshooting guide.\nAdditional functions for text extraction#\nhtml2txt\nand baseline\nfunctions offer simpler approaches to extracting text from HTML content, prioritizing performance over precision.\nhtml2txt()#\nhtml2txt\nfunction serves as a last resort for extracting text from HTML content. It emulates the behavior of similar functions in other packages and can be used to output all possible text from a given HTML source, maximizing recall. However, it may not always produce accurate or meaningful results, as it does not consider the context of the extracted sections.\n>>> from trafilatura import html2txt\n>>> html2txt(downloaded)\nbaseline()#\nbaseline\nfunction instead. This function returns a tuple containing an LXML element with the body, the extracted text as a string, and the length of the text. It uses a set of heuristics to extract text from the HTML content, which generally produces more accurate results than html2txt\n.\n>>> from trafilatura import baseline\n>>> postbody, text, len_text = baseline(downloaded)\nGuessing if text can be found#\nis_probably_readerable()\nhas been ported from Mozilla’s Readability.js, it is available from version 1.10 onwards and provides a way to guess if a page probably has a main text to extract.\n>>> from trafilatura.readability_lxml import is_probably_readerable\n>>> is_probably_readerable(html) # HTML string or already parsed tree\nLanguage identification#\n>>> result = extract(downloaded, url, target_language=\"de\")\nNote\npip install trafilatura[all]\n.\nThis feature currently uses the py3langid package and is dependent on language availability and performance of the original model.\nOptimizing for speed#\ntrafilatura[all]\n, htmldate[speed]\n), but also on the extraction strategy.\n# skip algorithms used as fallback\n>>> result = extract(downloaded, no_fallback=True)\nThe following combination usually leads to shorter processing times:\n>>> result = extract(downloaded, include_comments=False, include_tables=False, no_fallback=True)\nExtraction settings#\nFunction parameters#\nExtractor\nclass provides a convenient way to define and manage extraction parameters. It allows users to customize all options used by the extraction functions and offers a convenient shortcut compared to multiple function parameters.\nHere is how to use the class:\n# import the Extractor class from the settings module\n>>> from trafilatura.settings import Extractor\n# set multiple options at once\n>>> options = Extractor(output_format=\"json\", with_metadata=True)\n# add or adjust settings as needed\n>>> options.formatting = True # same as include_formatting\n>>> options.source = \"My Source\" # useful for debugging\n# use the options in an extraction function\n>>> extract(my_doc, options=options)\nSee the settings.py\nfile for a full example.\nMetadata extraction#\nwith_metadata=True\n: extract metadata fields and include them in the outputonly_with_metadata=True\n: only output documents featuring all essential metadata (date, title, url)\nDate#\nextract_metadata\nfunction in trafilatura.metadata\n, most notably:\nextensive_search\n(boolean), to activate further heuristics (higher recall, lower precision)original_date\n(boolean) to look for the original publication date,outputformat\n(string), to provide a custom datetime format,max_date\n(string), to set the latest acceptable date manually (YYYY-MM-DD format).\n# import the extract() function, use a previously downloaded document\n# pass the new parameters as dict\n>>> extract(downloaded, output_format=\"xml\", date_extraction_params={\n\"extensive_search\": True, \"max_date\": \"2018-07-01\"\n})\nURL#\n# define a URL and download the example\n>>> url = \"https://web.archive.org/web/20210613232513/https://www.thecanary.co/feature/2021/05/19/another-by-election-headache-is-incoming-for-keir-starmer/\"\n>>> downloaded = fetch_url(url)\n# content discarded since necessary metadata couldn't be extracted\n>>> bare_extraction(downloaded, only_with_metadata=True)\n>>>\n# date found in URL, extraction successful\n>>> bare_extraction(downloaded, only_with_metadata=True, url=url)\nMemory use#\n# import the function\n>>> from trafilatura.meta import reset_caches\n# use it at any given point\n>>> reset_caches()\nInput/Output types#\nPython objects as output#\nThe extraction can be customized using a series of parameters, for more see the core functions page.\nbare_extraction\ncan be used to bypass output conversion, it returns Python variables for metadata (dictionary) as well as main text and comments (both LXML objects).\n>>> from trafilatura import bare_extraction\n>>> bare_extraction(downloaded)\nRaw HTTP response objects#\nThe fetch_response()\nfunction can pass a response object straight to the extraction.\nresponse.url\nand then pass is directly as a URL argument to the extraction function:\n# necessary components\n>>> from trafilatura import fetch_response, bare_extraction\n# load an example\n>>> response = fetch_response(\"https://www.example.org\")\n# perform extract() or bare_extraction() on Trafilatura's response object\n>>> bare_extraction(response.data, url=response.url) # here is the redirection URL\nLXML objects#\n# define document and load it with LXML\n>>> from lxml import html\n>>> my_doc = \"\"\"<html><body><article><p>\nHere is the main text.\n</p></article></body></html>\"\"\"\n>>> mytree = html.fromstring(my_doc)\n# extract from the already loaded LXML tree\n>>> extract(mytree)\n'Here is the main text.'\nInteraction with BeautifulSoup#\nHere is how to convert a BS4 object to LXML format in order to use it with Trafilatura:\n>>> from bs4 import BeautifulSoup\n>>> from lxml.html.soupparser import convert_tree\n>>> from trafilatura import extract\n>>> soup = BeautifulSoup(\"<html><body><time>The date is Feb 2, 2024</time></body></html>\", \"lxml\")\n>>> lxml_tree = convert_tree(soup)[0]\n>>> extract(lxml_tree)\nDeprecations#\nThe following functions and arguments are deprecated:\n- extraction:\nprocess_record()\nfunction → useextract()\ninsteadcsv_output\n,json_output\n,tei_output\n,xml_output\n→ useoutput_format\nparameter insteadbare_extraction(as_dict=True)\n→ the function returns aDocument\nobject, use.as_dict()\nmethod on itbare_extraction()\nandextract()\n:no_fallback\n→ usefast\ninsteadmax_tree_size\nparameter moved tosettings.cfg\nfile\ndownloads:\ndecode\nargument infetch_url()\n→ usefetch_response\ninsteadutils:\ndecode_response()\nfunction → usedecode_file()\ninsteadwith_metadata\n(include metadata) had once the effect of today’sonly_with_metadata\n(only documents with necessary metadata)",
  "truncated": false,
  "blocked": false,
  "extractor": "trafilatura:precision",
  "quality_score": 87,
  "quality_metrics": {
    "char_len": 9361,
    "line_count": 231,
    "unique_line_ratio": 0.974,
    "noise_line_ratio": 0.0
  },
  "degraded": false
}