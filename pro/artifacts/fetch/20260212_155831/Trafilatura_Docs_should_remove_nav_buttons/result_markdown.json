{
  "success": true,
  "url": "https://trafilatura.readthedocs.io/en/latest/usage-python.html",
  "via_worker": false,
  "via_playwright": false,
  "status_code": 200,
  "markdown": "# With Python#\n\n## The Python programming language#\n\nPython can be easy to pick up whether you’re a first time programmer or you’re experienced with other languages:\n\n## Step-by-step#\n\n### Quickstart#\n\nFor the basics see quickstart documentation page.\n\n### Extraction functions#\n\nThe functions can be imported using `from trafilatura import ...`\n\nand used on raw documents (strings) or parsed HTML (LXML elements).\n\nMain text extraction, good balance between precision and recall:\n\n`extract`\n\n: Wrapper function, easiest way to perform text extraction and conversion`bare_extraction`\n\n: Internal function returning bare Python variables\n\nAdditional fallback functions:\n\n`baseline`\n\n: Faster extraction function targeting text paragraphs and/or JSON metadata`html2txt`\n\n: Extract all text in a document, maximizing recall\n\n### Output#\n\nBy default, the output is in plain text (TXT) format without metadata. The following additional formats are available:\n\nCSV\n\nHTML (from version 1.11 onwards)\n\nJSON\n\nMarkdown (from version 1.9 onwards)\n\nXML and XML-TEI (following the guidelines of the Text Encoding Initiative)\n\nTo specify the output format, use one of the following strings: `\"csv\", \"json\", \"html\", \"markdown\", \"txt\", \"xml\", \"xmltei\"`\n\n.\n\nThe `bare_extraction`\n\nfunction also accepts an additional `python`\n\nformat to work with Python on the output.\n\nTo extract and include metadata in the output, use the `with_metadata=True`\n\nargument.\n\n#### Examples#\n\n```\n# some formatting preserved in basic XML structure\n>>> extract(downloaded, output_format=\"xml\")\n# output in JSON format with metadata extracted\n>>> extract(downloaded, output_format=\"json\", with_metadata=True)\n```\n\nNote that combining TXT, CSV and JSON formats with certain structural elements (e.g. formatting or links) triggers output in Markdown format (plain text with additional elements).\n\n### Choice of HTML elements#\n\nCustomize the extraction process by including or excluding specific HTML elements:\n\n- Text elements:\n`include_comments=True`\n\nInclude comment sections at the bottom of articles.\n\n`include_tables=True`\n\nExtract text from HTML\n\n`<table>`\n\nelements.\n\n- Structural elements:\n`include_formatting=True`\n\nKeep structural elements related to formatting (\n\n`<b>`\n\n/`<strong>`\n\n,`<i>`\n\n/`<emph>`\n\netc.)`include_links=True`\n\nKeep link targets (in\n\n`href=\"...\"`\n\n)`include_images=True`\n\nKeep track of images along with their targets (\n\n`<img>`\n\nattributes: alt, src, title)\n\nTo operate on these elements, pass the corresponding parameters to the `extract()`\n\nfunction:\n\n```\n# exclude comments from the output\n>>> result = extract(downloaded, include_comments=False)\n# skip tables and include links in the output\n>>> result = extract(downloaded, include_tables=False, include_links=True)\n# convert relative links to absolute links where possible\n>>> extract(downloaded, output_format='xml', include_links=True, url=url)\n```\n\n#### Important notes#\n\n`include_comments`\n\nand`include_tables`\n\nare activated by default.Including extra elements works best with conversion to XML formats or using\n\n`bare_extraction()`\n\n. This allows for direct display and manipulation of the elements.Certain elements may not be visible in the output if the chosen format does not allow it.\n\nSelecting Markdown automatically includes text formatting.\n\nHint\n\nThe heuristics used by the main algorithm change according to the presence of certain elements in the HTML. If the output seems odd, try removing a constraint (e.g. formatting) to improve the result.\n\n### The precision and recall presets#\n\nThe main extraction functions offer two presets to adjust to focus of the extraction process: `favor_precision`\n\nand `favor_recall`\n\n.\n\nThese parameters allow you to change the balance between accuracy and comprehensiveness of the output.\n\n```\n>>> result = extract(downloaded, url, favor_precision=True)\n```\n\n#### Precision#\n\nIf your results contain too much noise, prioritize precision to focus on the most central and relevant elements.\n\nAdditionally, you can use the\n\n`prune_xpath`\n\nparameter to target specific HTML elements using a list of XPath expressions.\n\n#### Recall#\n\nIf parts of your documents are missing, try this preset to take more elements into account.\n\nIf content is still missing, refer to the troubleshooting guide.\n\n### Additional functions for text extraction#\n\nThe `html2txt`\n\nand `baseline`\n\nfunctions offer simpler approaches to extracting text from HTML content, prioritizing performance over precision.\n\n#### html2txt()#\n\nThe `html2txt`\n\nfunction serves as a last resort for extracting text from HTML content. It emulates the behavior of similar functions in other packages and can be used to output all possible text from a given HTML source, maximizing recall. However, it may not always produce accurate or meaningful results, as it does not consider the context of the extracted sections.\n\n```\n>>> from trafilatura import html2txt\n>>> html2txt(downloaded)\n```\n\n#### baseline()#\n\nFor a better balance between precision and recall, as well as improved performance, consider using the `baseline`\n\nfunction instead. This function returns a tuple containing an LXML element with the body, the extracted text as a string, and the length of the text. It uses a set of heuristics to extract text from the HTML content, which generally produces more accurate results than `html2txt`\n\n.\n\n```\n>>> from trafilatura import baseline\n>>> postbody, text, len_text = baseline(downloaded)\n```\n\nFor more advanced use cases, consider using other functions in the package that provide more control and customization over the text extraction process.\n\n### Guessing if text can be found#\n\nThe function `is_probably_readerable()`\n\nhas been ported from Mozilla’s Readability.js, it is available from version 1.10 onwards and provides a way to guess if a page probably has a main text to extract.\n\n```\n>>> from trafilatura.readability_lxml import is_probably_readerable\n>>> is_probably_readerable(html) # HTML string or already parsed tree\n```\n\n### Language identification#\n\nThe target language can also be set using 2-letter codes (ISO 639-1), there will be no output if the detected language of the result does not match and no such filtering if the identification component has not been installed (see above installation instructions) or if the target language is not available.\n\n```\n>>> result = extract(downloaded, url, target_language=\"de\")\n```\n\nNote\n\nAdditional components are required: `pip install trafilatura[all]`\n\n.\nThis feature currently uses the py3langid package and is dependent on language availability and performance of the original model.\n\n### Optimizing for speed#\n\nExecution speed not only depends on the platform and on supplementary packages (`trafilatura[all]`\n\n, `htmldate[speed]`\n\n), but also on the extraction strategy.\n\nThe available fallbacks make extraction more precise but also slower. The use of fallback algorithms can also be bypassed in *fast* mode, which should make extraction about twice as fast:\n\n```\n# skip algorithms used as fallback\n>>> result = extract(downloaded, no_fallback=True)\n```\n\nThe following combination usually leads to shorter processing times:\n\n```\n>>> result = extract(downloaded, include_comments=False, include_tables=False, no_fallback=True)\n```\n\n## Extraction settings#\n\n### Function parameters#\n\nStarting from version 1.9, the `Extractor`\n\nclass provides a convenient way to define and manage extraction parameters. It allows users to customize all options used by the extraction functions and offers a convenient shortcut compared to multiple function parameters.\n\nHere is how to use the class:\n\n```\n# import the Extractor class from the settings module\n>>> from trafilatura.settings import Extractor\n# set multiple options at once\n>>> options = Extractor(output_format=\"json\", with_metadata=True)\n# add or adjust settings as needed\n>>> options.formatting = True # same as include_formatting\n>>> options.source = \"My Source\" # useful for debugging\n# use the options in an extraction function\n>>> extract(my_doc, options=options)\n```\n\nSee the `settings.py`\n\nfile for a full example.\n\n### Metadata extraction#\n\n`with_metadata=True`\n\n: extract metadata fields and include them in the output`only_with_metadata=True`\n\n: only output documents featuring all essential metadata (date, title, url)\n\n#### Date#\n\nAmong metadata extraction, dates are handled by an external module: htmldate. By default, focus is on original dates and the extraction replicates the *fast/no_fallback* option.\n\nCustom parameters can be passed through the extraction function or through the `extract_metadata`\n\nfunction in `trafilatura.metadata`\n\n, most notably:\n\n`extensive_search`\n\n(boolean), to activate further heuristics (higher recall, lower precision)`original_date`\n\n(boolean) to look for the original publication date,`outputformat`\n\n(string), to provide a custom datetime format,`max_date`\n\n(string), to set the latest acceptable date manually (YYYY-MM-DD format).\n\n```\n# import the extract() function, use a previously downloaded document\n# pass the new parameters as dict\n>>> extract(downloaded, output_format=\"xml\", date_extraction_params={\n\"extensive_search\": True, \"max_date\": \"2018-07-01\"\n})\n```\n\n#### URL#\n\nEven if the page to process has already been downloaded it can still be useful to pass the URL as an argument. See this previous bug for an example:\n\n```\n# define a URL and download the example\n>>> url = \"https://web.archive.org/web/20210613232513/https://www.thecanary.co/feature/2021/05/19/another-by-election-headache-is-incoming-for-keir-starmer/\"\n>>> downloaded = fetch_url(url)\n# content discarded since necessary metadata couldn't be extracted\n>>> bare_extraction(downloaded, only_with_metadata=True)\n>>>\n# date found in URL, extraction successful\n>>> bare_extraction(downloaded, only_with_metadata=True, url=url)\n```\n\n### Memory use#\n\nTrafilatura uses caches to speed up extraction and cleaning processes. This may lead to memory leaks in some cases, particularly in large-scale applications. If that happens you can reset all cached information in order to release RAM:\n\n```\n# import the function\n>>> from trafilatura.meta import reset_caches\n# use it at any given point\n>>> reset_caches()\n```\n\n## Input/Output types#\n\n### Python objects as output#\n\nThe extraction can be customized using a series of parameters, for more see the core functions page.\n\nThe function `bare_extraction`\n\ncan be used to bypass output conversion, it returns Python variables for metadata (dictionary) as well as main text and comments (both LXML objects).\n\n```\n>>> from trafilatura import bare_extraction\n>>> bare_extraction(downloaded)\n```\n\n### Raw HTTP response objects#\n\nThe `fetch_response()`\n\nfunction can pass a response object straight to the extraction.\n\nThis can be useful to get the final redirection URL with `response.url`\n\nand then pass is directly as a URL argument to the extraction function:\n\n```\n# necessary components\n>>> from trafilatura import fetch_response, bare_extraction\n# load an example\n>>> response = fetch_response(\"https://www.example.org\")\n# perform extract() or bare_extraction() on Trafilatura's response object\n>>> bare_extraction(response.data, url=response.url) # here is the redirection URL\n```\n\n### LXML objects#\n\nThe input can consist of a previously parsed tree (i.e. a *lxml.html* object), which is then handled seamlessly:\n\n```\n# define document and load it with LXML\n>>> from lxml import html\n>>> my_doc = \"\"\"<html><body><article><p>\nHere is the main text.\n</p></article></body></html>\"\"\"\n>>> mytree = html.fromstring(my_doc)\n# extract from the already loaded LXML tree\n>>> extract(mytree)\n'Here is the main text.'\n```\n\n### Interaction with BeautifulSoup#\n\nHere is how to convert a BS4 object to LXML format in order to use it with Trafilatura:\n\n```\n>>> from bs4 import BeautifulSoup\n>>> from lxml.html.soupparser import convert_tree\n>>> from trafilatura import extract\n>>> soup = BeautifulSoup(\"<html><body><time>The date is Feb 2, 2024</time></body></html>\", \"lxml\")\n>>> lxml_tree = convert_tree(soup)[0]\n>>> extract(lxml_tree)\n```\n\n## Deprecations#\n\nThe following functions and arguments are deprecated:\n\n- extraction:\n`process_record()`\n\nfunction → use`extract()`\n\ninstead`csv_output`\n\n,`json_output`\n\n,`tei_output`\n\n,`xml_output`\n\n→ use`output_format`\n\nparameter instead`bare_extraction(as_dict=True)`\n\n→ the function returns a`Document`\n\nobject, use`.as_dict()`\n\nmethod on it`bare_extraction()`\n\nand`extract()`\n\n:`no_fallback`\n\n→ use`fast`\n\ninstead`max_tree_size`\n\nparameter moved to`settings.cfg`\n\nfile\n\ndownloads:\n\n`decode`\n\nargument in`fetch_url()`\n\n→ use`fetch_response`\n\ninsteadutils:\n\n`decode_response()`\n\nfunction → use`decode_file()`\n\ninsteadmetadata:\n\n`with_metadata`\n\n(include metadata) had once the effect of today’s`only_with_metadata`\n\n(only documents with necessary metadata)",
  "truncated": false,
  "blocked": false,
  "extractor": "trafilatura:precision",
  "quality_score": 100,
  "quality_metrics": {
    "char_len": 12839,
    "line_count": 281,
    "unique_line_ratio": 0.984,
    "noise_line_ratio": 0.0
  },
  "degraded": false
}